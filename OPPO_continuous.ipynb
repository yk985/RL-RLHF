{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45dd5a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%aimport -numpy\n",
    "%aimport -torch\n",
    "%aimport -gym\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (16, 10)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# for saving/loading\n",
    "import os\n",
    "\n",
    "import base64, io\n",
    "\n",
    "# For visualization\n",
    "from gym.wrappers.monitoring import video_recorder\n",
    "from IPython.display import HTML\n",
    "from IPython import display\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1866e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\turin\\OneDrive\\EPFL\\Master\\MA2\\RL\\RLHF\\Git RLHF\\RL-RLHF\\Plot_Functions.py:19: DeprecationWarning: invalid escape sequence '\\p'\n",
      "  plt.plot(updates1, reward_hist_pi1, label=f\"$\\pi_1$ Scores \\n mean = {np.mean(reward_hist_pi1):.1f}\")\n"
     ]
    }
   ],
   "source": [
    "from Train_policy_func import PolicyContinuous, device\n",
    "from Generate_traj_func import generate_trajectory\n",
    "from Plot_Functions import plot_suboptimality, plot_trajectory_performance, plot_Scores\n",
    "from PPO import ppo_update, RolloutBuffer, evaluate_policy\n",
    "\n",
    "# Training loop for OPPO\n",
    "from OPPO import baseline_CartPole_V0, baseline_MountainCar_continuous, OPPO_update\n",
    "# from OPPO import baseline_1, baseline_CartPole_v0_Fla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "709aa521",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\turin\\AppData\\Roaming\\Python\\Python311\\site-packages\\gym\\envs\\registration.py:601: UserWarning: \u001b[33mWARN: Using the latest versioned environment `MountainCarContinuous-v0` instead of the unversioned environment `MountainCarContinuous`.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\turin\\AppData\\Roaming\\Python\\Python311\\site-packages\\gym\\core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "C:\\Users\\turin\\AppData\\Roaming\\Python\\Python311\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Box' object has no attribute 'n'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 32\u001b[0m\n\u001b[0;32m     26\u001b[0m seed_training_policies  \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m42\u001b[39m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# ── Instantiate the two policies & optimizers for OPPO ──\u001b[39;00m\n\u001b[0;32m     31\u001b[0m pi1_oppo \u001b[38;5;241m=\u001b[39m PolicyContinuous(state_size\u001b[38;5;241m=\u001b[39menv_oppo\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m---> 32\u001b[0m                      action_size\u001b[38;5;241m=\u001b[39menv_oppo\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     33\u001b[0m opt1_oppo    \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(pi1_oppo\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr_oppo)\n\u001b[0;32m     35\u001b[0m pi2_oppo \u001b[38;5;241m=\u001b[39m PolicyContinuous(state_size\u001b[38;5;241m=\u001b[39menv_oppo\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m     36\u001b[0m                      action_size\u001b[38;5;241m=\u001b[39menv_oppo\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Box' object has no attribute 'n'"
     ]
    }
   ],
   "source": [
    "# Choose environment:\n",
    "\n",
    "# env 3:  MountainCarContinuous,\n",
    "\n",
    "env_name='MountainCarContinuous'\n",
    "env_oppo = gym.make(env_name)\n",
    "baseline = baseline_MountainCar_continuous\n",
    "state_dim  = env_oppo.observation_space.shape[0]  # e.g. 2\n",
    "action_dim = env_oppo.action_space.shape[0]       # e.g. 1\n",
    "policy = PolicyContinuous(state_dim, action_dim).to(device)\n",
    "target_score = 180 # TODO\n",
    "\n",
    "# pass bounds into act (optional)\n",
    "low, high = env_oppo.action_space.low, env_oppo.action_space.high\n",
    "\n",
    "\n",
    "load_policies = False\n",
    "\n",
    "# ── Hyperparams for OPPO ──\n",
    "lr_oppo = 0.003 # learning rate for OPPO\n",
    "n_episodes              = 1200 # keep same “number of iterations” for fair comparison\n",
    "max_t                   = 200  # cap on steps per episode\n",
    "print_every             = 20   # print every x episodes and avg also the score for every x episodes\n",
    "gamma                   = 0.99\n",
    "\n",
    "seed_training_policies  = 42\n",
    "\n",
    "\n",
    "# ── Instantiate the two policies & optimizers for OPPO ──\n",
    "\n",
    "pi1_oppo = PolicyContinuous(state_size=env_oppo.observation_space.shape[0],\n",
    "                     action_size=env_oppo.action_space.n).to(device)\n",
    "opt1_oppo    = torch.optim.Adam(pi1_oppo.parameters(), lr=lr_oppo)\n",
    "\n",
    "pi2_oppo = PolicyContinuous(state_size=env_oppo.observation_space.shape[0],\n",
    "                     action_size=env_oppo.action_space.n).to(device)\n",
    "opt2_oppo   = torch.optim.Adam(pi2_oppo.parameters(), lr=lr_oppo)\n",
    "\n",
    "\n",
    "# ── Load the policies if you want to reuse them ──\n",
    "if load_policies:\n",
    "  pi1_oppo.load_state_dict(torch.load(f\"pi1_oppo_{env_name}.pth\"))\n",
    "\n",
    "\n",
    "# ── Run the OPPO training ──\n",
    "if not load_policies:\n",
    "  # Training the policy:\n",
    "  scores_oppo = OPPO_update(\n",
    "      policy       = pi1_oppo,\n",
    "      optimizer    = opt1_oppo,\n",
    "      env          = env_oppo,\n",
    "      baseline     = baseline,\n",
    "      n_episodes   = n_episodes,\n",
    "      max_t        = max_t,\n",
    "      gamma        = gamma,      # you can reuse your PPO γ\n",
    "      print_every  = print_every,\n",
    "      early_stop   = False,\n",
    "      seed = seed_training_policies,\n",
    "      target_score = target_score,\n",
    "      env_name= env_name,\n",
    "      display_every = False\n",
    "  )\n",
    "\n",
    "pi2_oppo.load_state_dict(torch.load(f\"pi2_oppo_{env_name}.pth\"))\n",
    "\n",
    "\n",
    "seed_evaluation = 26\n",
    "num_episodes = 100\n",
    "pi1_mean_reward, pi1_reward = evaluate_policy(pi1_oppo, env_oppo, n_episodes=num_episodes)\n",
    "pi2_mean_reward, pi2_reward = evaluate_policy(pi2_oppo, env_oppo, n_episodes=num_episodes)\n",
    "print(\"Evaluations over\", num_episodes, \"episodes done for both policies\")\n",
    "\n",
    "plot_suboptimality(\n",
    "    pi1_reward,\n",
    "    pi2_reward,\n",
    "    max_reward=200, # for CartPole-v0\n",
    ")\n",
    "\n",
    "plot_Scores(\n",
    "    pi1_reward,\n",
    "    pi2_reward,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
