import math
from Generate_traj_func import generate_trajectory
import numpy as np
import torch

def sample_preference_pairs(pi1, pi2, env, K=100):
    pairs = []
    for _ in range(K):
        t1 = generate_trajectory(pi1, env)
        t2 = generate_trajectory(pi2, env)
        R1, R2 = sum(s['reward'] for s in t1), sum(s['reward'] for s in t2)
        p1 = math.exp(R1) / (math.exp(R1) + math.exp(R2))
        if np.random.uniform()<p1:
            pairs.append({
                "traj_acc": t1, "traj_rej": t2,
                "R_acc": R1, "R_rej": R2, "pref_prob": p1
            })
        else:
            pairs.append({
                "traj_acc": t2, "traj_rej": t1,
                "R_acc": R2, "R_rej": R1, "pref_prob": p1
            })
    return pairs


def log_policy_of_traj(trajectories):
    log_prob_traj=sum(traj["log_prob"] for traj in trajectories)
    return log_prob_traj


def extract_states_actions(trajectory, device="cpu"):
    """
    Extracts state and action tensors from a trajectory.

    Args:
        trajectory: List of dicts with keys: 'state', 'action', ...
        device: Torch device to move tensors to

    Returns:
        states: Tensor of shape [T, state_dim]
        actions: Tensor of shape [T]
    """
    states = torch.tensor([step["state"] for step in trajectory], dtype=torch.float32).to(device)
    actions = torch.tensor([step["action"] for step in trajectory], dtype=torch.long).to(device)
    return states, actions


def compute_logprob_trajectory(policy, trajectory, device="cpu"):# for trajectories generated by another policy
    """
    Args:
        policy: your Policy instance
        trajectory: trajectory we extract states and actions from

    Returns:
        logprob: scalar tensor = log π(τ) = sum_t log π(a_t | s_t)
    """
    states,actions=extract_states_actions(trajectory,device=device)
    pi, _ = policy(states)                     # pi: [T, action_size]
    dist = torch.distributions.Categorical(pi) # use categorical distribution
    log_probs = dist.log_prob(actions)         # log π(a_t | s_t) for each t
    return log_probs.sum()

def compute_reward_from_traj(reward_model,trajectory,device='cpu'):
    total_reward=0
    for state,action in extract_states_actions(trajectory,device=device):
        total_reward+=reward_model(state,action)
    return total_reward



