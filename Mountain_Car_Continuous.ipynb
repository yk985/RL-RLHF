{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgc7cvHkWmkZ"
      },
      "source": [
        "## Learning Notes of Renee. Article:\n",
        "https://colab.research.google.com/drive/1m5Ppsrv6B5maUJ-vMgbZtMeSxqFfUVSP?usp=sharing#scrollTo=9xd4fB8ZLcRT\n",
        "https://reneelin2019.medium.com/use-stable-baselines3-to-solve-mountain-car-continuous-in-gym-3216912cd5e3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6LmgCV2bUjPl"
      },
      "outputs": [],
      "source": [
        "# pip install stable-baselines3[extra]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9xd4fB8ZLcRT"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.ppo import MlpPolicy\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.evaluation import evaluate_policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RC4t0LguVJ2y"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tqdm\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LB554wcjU_3t"
      },
      "outputs": [],
      "source": [
        "# Saving logs to visulise in Tensorboard, saving models\n",
        "models_dir = f\"models/Mountain-{time.time()}\"\n",
        "logdir = f\"logs/Mountain-{time.time()}\"\n",
        "\n",
        "if not os.path.exists(models_dir):\n",
        "    os.makedirs(models_dir)\n",
        "\n",
        "if not os.path.exists(logdir):\n",
        "    os.makedirs(logdir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "reward_threshold_pi1 = 100\n",
        "reward_threshold_pi2 = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf4GQpUqUL0w",
        "outputId": "01bb3899-b427-429e-d6fc-2f6a3667cc66"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\turin\\AppData\\Roaming\\Python\\Python311\\site-packages\\stable_baselines3\\ppo\\ppo.py:155: UserWarning: You have specified a mini-batch size of 256, but because the `RolloutBuffer` is of size `n_steps * n_envs = 8`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 8\n",
            "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
            "Info: (n_steps=8 and n_envs=1)\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Parallel environments\n",
        "env = make_vec_env(\"MountainCarContinuous-v0\", n_envs=1)\n",
        "\n",
        "# The learning agent and hyperparameters\n",
        "model = PPO(\n",
        "    policy=MlpPolicy,\n",
        "    env=env,\n",
        "    seed=0,\n",
        "    batch_size=256,\n",
        "    ent_coef=0.00429,\n",
        "    learning_rate=7.77e-05,\n",
        "    n_epochs=10,\n",
        "    n_steps=8,\n",
        "    gae_lambda=0.9,\n",
        "    gamma=0.9999,\n",
        "    clip_range=0.1,\n",
        "    max_grad_norm =5,\n",
        "    vf_coef=0.19,\n",
        "    use_sde=True,\n",
        "    policy_kwargs=dict(log_std_init=-3.29, ortho_init=False),\n",
        "    verbose=0,\n",
        "    tensorboard_log=logdir\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9XQyvVnOVVCG",
        "outputId": "debffc0c-e435-4b26-93fb-943c74eb6f76"
      },
      "outputs": [],
      "source": [
        "#Training and saving models along the way\n",
        "TIMESTEPS = 20000\n",
        "for i in range(10):\n",
        "    model.learn(total_timesteps=TIMESTEPS,reset_num_timesteps=False, tb_log_name=\"PPO\")\n",
        "    # model.save(f\"{models_dir}/{TIMESTEPS*i}\")\n",
        "  \n",
        "    if i % 200 == 0:\n",
        "        mean_reward, std_reward = evaluate_policy(\n",
        "            model,\n",
        "            env,\n",
        "            n_eval_episodes=100,\n",
        "            deterministic=True,\n",
        "        )\n",
        "        if mean_reward > reward_threshold_pi2 and mean_reward < reward_threshold_pi1:\n",
        "            print(f\"Saving model pi2 at {mean_reward}\")\n",
        "            # model.save(f\"{models_dir}/pi2-{TIMESTEPS*i}\")\n",
        "            model.save(\"pi2\") \n",
        "        elif mean_reward > reward_threshold_pi1:\n",
        "            print(f\"Saving model pi1 at {mean_reward}\")\n",
        "            # model.save(f\"{models_dir}/pi1-{TIMESTEPS*i}\")\n",
        "            model.save(\"pi1\")   \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final mean reward: -99.89999999999998 +/- 2.842170943040401e-14\n"
          ]
        }
      ],
      "source": [
        "model.save(\"final_model\")\n",
        "mean_reward, std_reward = evaluate_policy(\n",
        "    model,\n",
        "    env,\n",
        "    n_eval_episodes=100,\n",
        "    deterministic=True,\n",
        ")\n",
        "print(f\"Final mean reward: {mean_reward} +/- {std_reward}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "uYVxU3RtUUXY",
        "outputId": "ad115f02-81b5-4286-e637-3d2ceb9aff90"
      },
      "outputs": [],
      "source": [
        "# # Check model performance\n",
        "# # load the best model you observed from tensorboard - the one reach the goal/ obtaining highest return\n",
        "# models_dir = \"models/Mountain-1653282767.3143597\"\n",
        "# model_path = f\"{models_dir}/80000\"\n",
        "# best_model = PPO.load(model_path, env=env)\n",
        "\n",
        "# obs = env.reset()\n",
        "# while True:\n",
        "#     action, _states = best_model.predict(obs)\n",
        "#     obs, rewards, dones, info = env.step(action)\n",
        "#     # env.render()  use Python IDE to check, I havn't figure out how to render in Notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQMQzFBidum-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
