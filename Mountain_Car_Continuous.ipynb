{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgc7cvHkWmkZ"
      },
      "source": [
        "## Learning Notes of Renee. Article:\n",
        "https://colab.research.google.com/drive/1m5Ppsrv6B5maUJ-vMgbZtMeSxqFfUVSP?usp=sharing#scrollTo=9xd4fB8ZLcRT\n",
        "https://reneelin2019.medium.com/use-stable-baselines3-to-solve-mountain-car-continuous-in-gym-3216912cd5e3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "6LmgCV2bUjPl"
      },
      "outputs": [],
      "source": [
        "# pip install stable-baselines3[extra]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "9xd4fB8ZLcRT"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.ppo import MlpPolicy\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.evaluation import evaluate_policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "RC4t0LguVJ2y"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tqdm\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "LB554wcjU_3t"
      },
      "outputs": [],
      "source": [
        "# Saving logs to visulise in Tensorboard, saving models\n",
        "models_dir = f\"models/Mountain-{time.time()}\"\n",
        "logdir = f\"logs/Mountain-{time.time()}\"\n",
        "\n",
        "if not os.path.exists(models_dir):\n",
        "    os.makedirs(models_dir)\n",
        "\n",
        "if not os.path.exists(logdir):\n",
        "    os.makedirs(logdir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "reward_threshold_pi1 = -190\n",
        "reward_threshold_pi2 = -500\n",
        "\n",
        "seed = 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf4GQpUqUL0w",
        "outputId": "01bb3899-b427-429e-d6fc-2f6a3667cc66"
      },
      "outputs": [],
      "source": [
        "# Parallel environments\n",
        "env = make_vec_env(\"Pendulum-v1\", n_envs=1, # MountainCarContinuous-v0\n",
        "                      seed=seed) #, env_kwargs={\"render_mode\": \"human\"},  max_episode_steps=2000,\n",
        "\n",
        "# The learning agent and hyperparameters\n",
        "\n",
        "model = PPO(\n",
        "#   n_envs= 4,\n",
        "#   n_timesteps= 1.0e5,\n",
        "  env=env,\n",
        "  policy= 'MlpPolicy',\n",
        "  n_steps= 1024,\n",
        "  gae_lambda= 0.95,\n",
        "  gamma= 0.9,\n",
        "  n_epochs= 10,\n",
        "#   batch_size= 64,\n",
        "  ent_coef= 0.0,\n",
        "  learning_rate=  1.0e-3,   \n",
        "  clip_range= 0.2,\n",
        "  use_sde= True,\n",
        "  sde_sample_freq= 4\n",
        "  \n",
        ")\n",
        "    \n",
        "\n",
        "# model = PPO(\n",
        "#     policy=MlpPolicy,\n",
        "#     env=env,\n",
        "#     seed=0,\n",
        "#     batch_size=256,\n",
        "#     ent_coef= 0.00429,\n",
        "#     learning_rate=7.77e-05,\n",
        "#     n_epochs=10,\n",
        "#     n_steps=8,\n",
        "#     gae_lambda=0.9,\n",
        "#     gamma=0.9999,\n",
        "#     clip_range=0.1,\n",
        "#     max_grad_norm =5,\n",
        "#     vf_coef= 0.19,\n",
        "#     use_sde=True,\n",
        "#     policy_kwargs=dict(log_std_init=-3.29, ortho_init=False),\n",
        "#     verbose=0,\n",
        "#     tensorboard_log=logdir\n",
        "#     )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9XQyvVnOVVCG",
        "outputId": "debffc0c-e435-4b26-93fb-943c74eb6f76"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[40], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m TIMESTEPS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100000\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m25\u001b[39m):\n\u001b[1;32m----> 4\u001b[0m     model\u001b[38;5;241m.\u001b[39mlearn(total_timesteps\u001b[38;5;241m=\u001b[39mTIMESTEPS,reset_num_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, tb_log_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPPO\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# model.save(f\"{models_dir}/{TIMESTEPS*i}\")\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\stable_baselines3\\ppo\\ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mlearn(\n\u001b[0;32m    312\u001b[0m         total_timesteps\u001b[38;5;241m=\u001b[39mtotal_timesteps,\n\u001b[0;32m    313\u001b[0m         callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[0;32m    314\u001b[0m         log_interval\u001b[38;5;241m=\u001b[39mlog_interval,\n\u001b[0;32m    315\u001b[0m         tb_log_name\u001b[38;5;241m=\u001b[39mtb_log_name,\n\u001b[0;32m    316\u001b[0m         reset_num_timesteps\u001b[38;5;241m=\u001b[39mreset_num_timesteps,\n\u001b[0;32m    317\u001b[0m         progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar,\n\u001b[0;32m    318\u001b[0m     )\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:337\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    334\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mep_info_buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    335\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdump_logs(iteration)\n\u001b[1;32m--> 337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    339\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\stable_baselines3\\ppo\\ppo.py:275\u001b[0m, in \u001b[0;36mPPO.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;66;03m# Optimization step\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 275\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    276\u001b[0m \u001b[38;5;66;03m# Clip grad norm\u001b[39;00m\n\u001b[0;32m    277\u001b[0m th\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_grad_norm)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    583\u001b[0m )\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[0;32m    348\u001b[0m     tensors,\n\u001b[0;32m    349\u001b[0m     grad_tensors_,\n\u001b[0;32m    350\u001b[0m     retain_graph,\n\u001b[0;32m    351\u001b[0m     create_graph,\n\u001b[0;32m    352\u001b[0m     inputs,\n\u001b[0;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    355\u001b[0m )\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#Training and saving models along the way\n",
        "TIMESTEPS = 100000\n",
        "for i in range(25):\n",
        "    model.learn(total_timesteps=TIMESTEPS,reset_num_timesteps=False, tb_log_name=\"PPO\")\n",
        "    # model.save(f\"{models_dir}/{TIMESTEPS*i}\")\n",
        "  \n",
        "    if i % 1 == 0:\n",
        "        mean_reward, std_reward = evaluate_policy(\n",
        "            model,\n",
        "            env,\n",
        "            n_eval_episodes=25,\n",
        "            deterministic=True,\n",
        "        )\n",
        "        print(f\" at update {i}: Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
        "        if mean_reward > reward_threshold_pi2 and mean_reward < -450:\n",
        "            print(f\"Saving model pi2 at {mean_reward}\")\n",
        "            # model.save(f\"{models_dir}/pi2-{TIMESTEPS*i}\")\n",
        "            model.save(\"pi2\") \n",
        "        elif mean_reward > reward_threshold_pi1:\n",
        "            print(f\"Saving model pi1 at {mean_reward}\")\n",
        "            # model.save(f\"{models_dir}/pi1-{TIMESTEPS*i}\")\n",
        "            model.save(\"pi1\")   \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " at update 24: Mean reward: -172.15 +/- 103.49\n"
          ]
        }
      ],
      "source": [
        "mean_reward, std_reward = evaluate_policy(\n",
        "    model,\n",
        "    env,\n",
        "    n_eval_episodes=25,\n",
        "    deterministic=True,\n",
        ")\n",
        "print(f\" at update {i}: Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model.save(\"final_model\")\n",
        "# mean_reward, std_reward = evaluate_policy(\n",
        "#     model,\n",
        "#     env,\n",
        "#     n_eval_episodes=100,\n",
        "#     deterministic=True,\n",
        "# )\n",
        "# print(f\"Final mean reward: {mean_reward} +/- {std_reward}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "uYVxU3RtUUXY",
        "outputId": "ad115f02-81b5-4286-e637-3d2ceb9aff90"
      },
      "outputs": [],
      "source": [
        "# # Check model performance\n",
        "# # load the best model you observed from tensorboard - the one reach the goal/ obtaining highest return\n",
        "# models_dir = \"models/Mountain-1653282767.3143597\"\n",
        "# model_path = f\"{models_dir}/80000\"\n",
        "# best_model = PPO.load(model_path, env=env)\n",
        "\n",
        "# obs = env.reset()\n",
        "# while True:\n",
        "#     action, _states = best_model.predict(obs)\n",
        "#     obs, rewards, dones, info = env.step(action)\n",
        "#     # env.render()  use Python IDE to check, I havn't figure out how to render in Notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQMQzFBidum-"
      },
      "outputs": [],
      "source": [
        "# env.close()\n",
        "# model = None     # drop references to free logger file handles\n",
        "\n",
        "#in terminal?\n",
        "# python train.py && rm -rf logs/ models/\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
