{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1aa6520",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d6953a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "from OPPO import OPPO_update\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def OPPO_update(policy,\n",
    "                optimizer,\n",
    "                env,\n",
    "                baseline=None,\n",
    "                n_episodes=1000,\n",
    "                max_t=1000,\n",
    "                gamma=1.0,\n",
    "                print_every=100,\n",
    "                early_stop=False,\n",
    "                seed=42,\n",
    "                target_score=None,\n",
    "                env_name=\"CartPole-v0\",\n",
    "                display_every=False):\n",
    "\n",
    "    set_seed(seed, env)\n",
    "    checkpoint_reached = False\n",
    "    scores_deque = deque(maxlen=print_every)\n",
    "    scores = []\n",
    "\n",
    "    for e in range(1, n_episodes + 1):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        states = []\n",
    "\n",
    "        state = env.reset()\n",
    "        for step_in_episode in range(max_t):\n",
    "            states.append(state)\n",
    "            action, log_prob, _ = policy.act(state)\n",
    "            saved_log_probs.append(log_prob)\n",
    "\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        total_R = sum(rewards)\n",
    "        scores.append(total_R)\n",
    "        scores_deque.append(total_R)\n",
    "\n",
    "        discounts = [gamma ** i for i in range(len(rewards))]\n",
    "        rewards_to_go = [\n",
    "            sum(discounts[k] * rewards[k + t] for k in range(len(rewards) - t))\n",
    "            for t in range(len(rewards))\n",
    "        ]\n",
    "\n",
    "        # ======> Fit baseline here\n",
    "        if baseline is not None and hasattr(baseline, 'fit'):\n",
    "            baseline.fit(states, rewards_to_go)\n",
    "\n",
    "        # Compute policy loss\n",
    "        policy_loss_terms = []\n",
    "        for log_prob, G, s in zip(saved_log_probs, rewards_to_go, states):\n",
    "            b = baseline(s) if baseline is not None else 0\n",
    "            advantage = G - b\n",
    "            policy_loss_terms.append(-log_prob * advantage)\n",
    "\n",
    "        policy_loss = torch.stack(policy_loss_terms).sum()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Logging\n",
    "        if e % print_every == 0:\n",
    "            avg_score = float(np.mean(scores_deque))\n",
    "            print(f\"Episode {e} \\t Average Score over last {print_every}: {avg_score:.1f}\")\n",
    "\n",
    "        if not checkpoint_reached and target_score is not None and np.mean(scores_deque) >= target_score / 2:\n",
    "            print(f\"Saving pi2 checkpoint at episode {e}...\")\n",
    "            torch.save(policy.state_dict(), f\"./Policies/pi2_ref_{env_name}_seed_{seed}.pth\")\n",
    "            checkpoint_reached = True\n",
    "\n",
    "        if target_score is not None and np.mean(scores_deque) >= target_score:\n",
    "            print(f\"Target score reached at episode {e}!\")\n",
    "            torch.save(policy.state_dict(), f\"./Policies/pi1_ref_{env_name}_seed_{seed}.pth\")\n",
    "            break\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def set_seed(seed, env):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    env.seed(seed)\n",
    "    env.action_space.seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_size=2, action_size=3, hidden_size=128):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, action_size),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        probs = self.net(state)\n",
    "        return probs\n",
    "\n",
    "    def act(self, state):\n",
    "        probs = self.forward(state)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        return action.item(), dist.log_prob(action), probs.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "\n",
    "class LinearBaseline(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(state_dim, 1, bias=True)\n",
    "\n",
    "    def forward(self, state):\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "        return self.linear(state).squeeze()\n",
    "\n",
    "    def fit(self, states, returns):\n",
    "        \"\"\"\n",
    "        Fit linear model: return = w·state + b using least-squares.\n",
    "        \"\"\"\n",
    "        states = torch.tensor(states, dtype=torch.float32, device=device)\n",
    "        returns = torch.tensor(returns, dtype=torch.float32, device=device)\n",
    "\n",
    "        # Add bias term manually (for least squares)\n",
    "        X = torch.cat([states, torch.ones(states.shape[0], 1, device=device)], dim=1)  # [N, D+1]\n",
    "        y = returns.view(-1, 1)  # [N, 1]\n",
    "\n",
    "        # Least squares solution: w = (X^T X)^-1 X^T y\n",
    "        try:\n",
    "            XtX = X.T @ X\n",
    "            Xty = X.T @ y\n",
    "            weights = torch.linalg.solve(XtX, Xty)  # [D+1, 1]\n",
    "        except RuntimeError:\n",
    "            print(\"Least squares failed due to singular matrix.\")\n",
    "            return\n",
    "\n",
    "        # Update model parameters\n",
    "        with torch.no_grad():\n",
    "            self.linear.weight.data = weights[:-1].T\n",
    "            self.linear.bias.data = weights[-1].squeeze()\n",
    "\n",
    "class PositionBasedBaseline:\n",
    "    def __init__(self, scale=10.0):\n",
    "        self.scale = scale  # Encourages stronger incentives to go right\n",
    "\n",
    "    def __call__(self, state):\n",
    "        position = state[0]  # position is the first state variable\n",
    "        # Shift range to [0, 1.7] and scale\n",
    "        return self.scale * (np.abs(position) + 1.2)  # maps [-1.2, 0.5] → [0, ~1.7*scale]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dec56ef2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m baseline \u001b[38;5;241m=\u001b[39m LinearBaseline(state_dim)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      8\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(policy\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3e-2\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m scores \u001b[38;5;241m=\u001b[39m OPPO_update(\n\u001b[0;32m     11\u001b[0m     policy\u001b[38;5;241m=\u001b[39mpolicy,\n\u001b[0;32m     12\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[0;32m     13\u001b[0m     env\u001b[38;5;241m=\u001b[39menv,\n\u001b[0;32m     14\u001b[0m     baseline\u001b[38;5;241m=\u001b[39mbaseline,\n\u001b[0;32m     15\u001b[0m     n_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[0;32m     16\u001b[0m     max_t\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m,\n\u001b[0;32m     17\u001b[0m     gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.89\u001b[39m,\n\u001b[0;32m     18\u001b[0m     print_every\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[0;32m     19\u001b[0m     early_stop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     20\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[0;32m     21\u001b[0m     target_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m110\u001b[39m,   \u001b[38;5;66;03m# around the solved threshold for MountainCar\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     env_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMountainCar-v0\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     23\u001b[0m     display_every\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     24\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\flako\\Desktop\\MA2\\RL\\Cloned_Project\\RL-RLHF\\OPPO.py:192\u001b[0m, in \u001b[0;36mOPPO_update\u001b[1;34m(policy, optimizer, env, baseline, n_episodes, max_t, gamma, print_every, early_stop, seed, target_score, env_name, display_every)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# gradient step\u001b[39;00m\n\u001b[0;32m    191\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 192\u001b[0m policy_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    193\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# logging\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\flako\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    628\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\flako\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[0;32m    348\u001b[0m     tensors,\n\u001b[0;32m    349\u001b[0m     grad_tensors_,\n\u001b[0;32m    350\u001b[0m     retain_graph,\n\u001b[0;32m    351\u001b[0m     create_graph,\n\u001b[0;32m    352\u001b[0m     inputs,\n\u001b[0;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    355\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\flako\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "policy = PolicyNetwork(state_dim, action_dim).to(device)\n",
    "baseline = LinearBaseline(state_dim).to(device)\n",
    "\n",
    "optimizer = optim.Adam(policy.parameters(), lr=3e-2)\n",
    "\n",
    "scores = OPPO_update(\n",
    "    policy=policy,\n",
    "    optimizer=optimizer,\n",
    "    env=env,\n",
    "    baseline=baseline,\n",
    "    n_episodes=1000,\n",
    "    max_t=300,\n",
    "    gamma=0.89,\n",
    "    print_every=50,\n",
    "    early_stop=True,\n",
    "    seed=32,\n",
    "    target_score=-110,   # around the solved threshold for MountainCar\n",
    "    env_name=\"MountainCar-v0\",\n",
    "    display_every=False\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119ea5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50 \t Average Score over the last 50 episodes: -600.0\n"
     ]
    }
   ],
   "source": [
    "from OPPO import OPPO_update\n",
    "\n",
    "class EscapePitBaseline:\n",
    "    def __init__(self, pit_center=-0.5, pit_width=0.2, vel_threshold=0.01, scale=10.0):\n",
    "        self.pit_center = pit_center\n",
    "        self.pit_width = pit_width\n",
    "        self.vel_threshold = vel_threshold\n",
    "        self.scale = scale\n",
    "\n",
    "    def __call__(self, state):\n",
    "        position, velocity = state\n",
    "        # Gaussian-like penalty for being in the pit\n",
    "        in_pit = np.exp(-((position - self.pit_center) ** 2) / (2 * self.pit_width ** 2))\n",
    "        low_velocity_penalty = np.exp(-abs(velocity) / self.vel_threshold)\n",
    "        # Product gives high penalty when in pit and not moving\n",
    "        penalty = in_pit * low_velocity_penalty\n",
    "        return self.scale * penalty\n",
    "\n",
    "class EscapePitBaseline:\n",
    "    def __init__(self, pit_center=-0.5, pit_width=0.2, speed_thresh=0.01, scale=10.0):\n",
    "        self.pit_center = pit_center\n",
    "        self.pit_width = pit_width\n",
    "        self.speed_thresh = speed_thresh\n",
    "        self.scale = scale\n",
    "\n",
    "    def __call__(self, state):\n",
    "        pos, vel = state\n",
    "        # Close to the pit?\n",
    "        in_pit = np.exp(-((pos - self.pit_center) ** 2) / (2 * self.pit_width ** 2))\n",
    "        # Not moving much (low speed, not velocity sign!)\n",
    "        low_speed = np.exp(-abs(vel) / self.speed_thresh)\n",
    "\n",
    "        # Only penalize being stuck, not moving\n",
    "        stuck_penalty = in_pit * low_speed\n",
    "\n",
    "        return self.scale * stuck_penalty\n",
    "\n",
    "baseline = EscapePitBaseline(scale=10.0)\n",
    "\n",
    "gym.envs.register(\n",
    "    id='MountainCarMyEasyVersion-v0',\n",
    "    entry_point='gym.envs.classic_control:MountainCarEnv',\n",
    "    max_episode_steps=600,      # MountainCar-v0 uses 200\n",
    "    reward_threshold=-110.0,\n",
    ")\n",
    "env = gym.make('MountainCarMyEasyVersion-v0')\n",
    "\n",
    "\n",
    "# env = gym.make(\"MountainCar-v0\")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "policy = PolicyNetwork(state_dim, action_dim).to(device)\n",
    "\n",
    "optimizer = optim.Adam(policy.parameters(), lr=4e-3)\n",
    "\n",
    "scores = OPPO_update(\n",
    "    policy=policy,\n",
    "    optimizer=optimizer,\n",
    "    env=env,\n",
    "    baseline=baseline,\n",
    "    n_episodes=1000,\n",
    "    max_t=600,\n",
    "    gamma=0.89,\n",
    "    print_every=50,\n",
    "    early_stop=True,\n",
    "    seed=32,\n",
    "    target_score=-110,   # around the solved threshold for MountainCar\n",
    "    env_name=\"MountainCar-v0\",\n",
    "    display_every=True\n",
    ")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
