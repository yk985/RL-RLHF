{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4293089f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gym\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144ef3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your PPO implementation\n",
    "from PPO import RolloutBuffer, ppo_update, evaluate_policy, device  # citeturn3file0\n",
    "from Train_policy_func import Policy, Policy_v2  # citeturn2file3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bf7cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86afcae0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "ENV_ID = \"CartPole-v0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dc76c3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    env = gym.make(ENV_ID)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbe2a87",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Define the search space for PPO hyperparameters\n",
    "dim_space = [\n",
    "    Real(1e-5, 1e-2, \"log-uniform\", name=\"learning_rate\"),\n",
    "    Real(0.9, 0.999, name=\"gamma\"),\n",
    "    Real(0.8, 0.99, name=\"gae_lambda\"),\n",
    "    Real(0.1, 0.3, name=\"clip_eps\"),\n",
    "    Real (0.0, 1, name=\"value_coef\"),\n",
    "    Real(0.0, 0.05, name=\"entropy_coef\"),\n",
    "    Integer(64, 512, name=\"n_steps\"),\n",
    "    Integer(2, 10, name=\"epochs\"),\n",
    "    Integer(32, 256, name=\"batch_size\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36020e07",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@use_named_args(dim_space)\n",
    "def objective(learning_rate, gamma, gae_lambda, clip_eps, value_coef, entropy_coef,\n",
    "              n_steps, epochs, batch_size):\n",
    "    \"\"\"\n",
    "    Train a PPO agent with given hyperparameters and return negative mean reward.\n",
    "    \"\"\"\n",
    "    # New policy and optimizer per trial\n",
    "    policy = Policy_v2().to(device)\n",
    "    optimizer = torch.optim.Adam(policy.parameters(), lr=learning_rate)\n",
    "    buffer = RolloutBuffer()\n",
    "\n",
    "    env = make_env()\n",
    "    obs = env.reset(seed=42)\n",
    "    total_timesteps = 2000\n",
    "    steps = 0\n",
    "\n",
    "    # Collect rollouts and update until budget exhausted\n",
    "    while steps < total_timesteps:\n",
    "        for _ in range(n_steps):\n",
    "            action, logp, value = policy.act(obs)\n",
    "            next_obs, reward, done, _ = env.step(action)\n",
    "            buffer.store(obs, action, logp, reward, torch.tensor(value), done)\n",
    "            obs = next_obs\n",
    "            steps += 1\n",
    "            if done:\n",
    "                obs = env.reset()\n",
    "        # Perform PPO update\n",
    "        ppo_update(\n",
    "            policy, optimizer, buffer,\n",
    "            gamma=gamma,\n",
    "            lam=gae_lambda,\n",
    "            c1=value_coef,\n",
    "            c2=entropy_coef,\n",
    "            clip_eps=clip_eps,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "\n",
    "    # Evaluate performance\n",
    "    mean_reward, _ = evaluate_policy(policy, env, n_episodes=5)\n",
    "    # We minimize the negative of performance\n",
    "    return -mean_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eb3bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Run Bayesian optimization\n",
    "    result = gp_minimize(\n",
    "        func=objective,\n",
    "        dimensions=dim_space,\n",
    "        n_calls=20,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Best hyperparameters\n",
    "    best_params = {dim.name: val for dim, val in zip(dim_space, result.x)}\n",
    "    print(\"Best hyperparameters found:\")\n",
    "    for key, val in best_params.items():\n",
    "        print(f\"  {key}: {val}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59e38e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Optionally, retrain a final model on full budget\n",
    "# print(\"Retraining final model with best hyperparameters...\")\n",
    "# policy = Policy().to(device)\n",
    "# optimizer = torch.optim.Adam(policy.parameters(), lr=best_params['learning_rate'])\n",
    "# buffer = RolloutBuffer()\n",
    "# env = make_env()\n",
    "# obs = env.reset(seed=42)\n",
    "# steps = 0\n",
    "# total_timesteps = 5000  # more timesteps for final training\n",
    "# while steps < total_timesteps:\n",
    "#     for _ in range(best_params['n_steps']):\n",
    "#         action, logp, value = policy.act(obs)\n",
    "#         next_obs, reward, done, _ = env.step(action)\n",
    "#         buffer.store(obs, action, logp, reward, torch.tensor(value), done)\n",
    "#         obs = next_obs\n",
    "#         steps += 1\n",
    "#         if done:\n",
    "#             obs = env.reset()\n",
    "#     ppo_update(\n",
    "#         policy, optimizer, buffer,\n",
    "#         gamma=best_params['gamma'],\n",
    "#         lam=best_params['gae_lambda'],\n",
    "#         c1=0.5,\n",
    "#         c2=best_params['entropy_coef'],\n",
    "#         clip_eps=best_params['clip_eps'],\n",
    "#         epochs=best_params['epochs'],\n",
    "#         batch_size=best_params['batch_size']\n",
    "#     )\n",
    "\n",
    "# # Final evaluation\n",
    "# mean_reward, rewards = evaluate_policy(policy, env, n_episodes=10)\n",
    "# print(f\"Final mean reward: {mean_reward}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "ppo-tune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
