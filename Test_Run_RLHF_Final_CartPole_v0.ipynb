{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1aa6520",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea82d443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (16, 10)\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import Adam\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387214c2",
   "metadata": {},
   "source": [
    "Our functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50b97559",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Train_policy_func import Policy, load_policy, evaluate_all_policies\n",
    "from Generate_traj_func import generate_trajectory\n",
    "from Plot_Functions import plot_suboptimality, plot_trajectory_performance, plot_Scores, plot_suboptimality_three_policies, plot_scores_RLHF\n",
    "from OPPO import baseline_CartPole_v0_Fla, OPPO_update, set_seed\n",
    "from PPO import evaluate_policy\n",
    "from pairs_generator import sample_preference_pairs\n",
    "from RLHF import RewardModel, train_policy_from_rollouts_n_updates\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ef4923",
   "metadata": {},
   "source": [
    "beta by default 0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d9ff4c",
   "metadata": {},
   "source": [
    "## Step 1:\n",
    "Creating the policies of references, here for three different seeds.\n",
    "\n",
    "The policies, for CartPole at least are optimised with an OPPO algorithm.\n",
    "They are saved as \"pi1\\_ref\\_{env_name}\\_seed\\_{seed}.pth\" and \"pi2\\_ref\\_{env_name}\\_seed\\_{seed}.pth\"\n",
    "\n",
    "For Pendulum, do not know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be3df14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:37: DeprecationWarning: invalid escape sequence '\\p'\n",
      "c:\\Users\\flako\\anaconda3\\Lib\\site-packages\\gym\\envs\\registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\flako\\anaconda3\\Lib\\site-packages\\gym\\core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\flako\\anaconda3\\Lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading $\\pi_{ref}$ 1|2 with seed 35\n",
      "Loading $\\pi_{ref}$ 1|2 with seed 42\n",
      "Loading $\\pi_{ref}$ 1|2 with seed 100\n",
      "=================================\n",
      "Evaluating pi_1 and pi_2 with seed=35 over 100 episodes\n",
      "Evaluations over 100 episodes done for both policies, using the seed {seed}\n",
      "pi_1(35) \t mean reward: \t 186.93 ± 24.03\n",
      "pi_2(35) \t mean reward: \t 99.91 ± 53.53\n",
      "------------------------------------------------------\n",
      "Evaluating pi_1 and pi_2 with seed=42 over 100 episodes\n",
      "Evaluations over 100 episodes done for both policies, using the seed {seed}\n",
      "pi_1(42) \t mean reward: \t 183.51 ± 40.43\n",
      "pi_2(42) \t mean reward: \t 127.82 ± 50.95\n",
      "------------------------------------------------------\n",
      "Evaluating pi_1 and pi_2 with seed=100 over 100 episodes\n",
      "Evaluations over 100 episodes done for both policies, using the seed {seed}\n",
      "pi_1(100) \t mean reward: \t 187.64 ± 28.03\n",
      "pi_2(100) \t mean reward: \t 115.26 ± 49.30\n",
      "------------------------------------------------------\n",
      "=================================\n"
     ]
    }
   ],
   "source": [
    "# env 1: CartPole-v0\n",
    "env_name='CartPole-v0'\n",
    "env = gym.make(env_name)\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "baseline = baseline_CartPole_v0_Fla\n",
    "target_score = 185 # for CartPole-v0\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# ── Hyperparams for OPPO ──\n",
    "lr_oppo                 = 0.003\n",
    "n_episodes              = 1200 # keep same “number of iterations” for fair comparison\n",
    "max_t                   = 200  # cap on steps per episode\n",
    "print_every             = 20   # print every x episodes and avg also the score for every x episodes\n",
    "gamma                   = 0.99\n",
    "\n",
    "\n",
    "Policy_list = []\n",
    "load_policies = True\n",
    "plot_scores = False\n",
    "n_eval = 100\n",
    "\n",
    "seeds_list=[35, 42, 100]\n",
    "\n",
    "for seed in seeds_list:\n",
    "    set_seed(seed, env)\n",
    "\n",
    "    if load_policies:\n",
    "        print(fr\"Loading pi_1 and pi_2 with {seed=}\")\n",
    "        pi_1 = load_policy(f\"pi1_ref_{env_name}_seed_{seed}.pth\", obs_dim, action_dim, device)\n",
    "        pi_2 = load_policy(f\"pi2_ref_{env_name}_seed_{seed}.pth\", obs_dim, action_dim, device)\n",
    "\n",
    "    else:\n",
    "        print(f\"Training pi_1 and pi_2 with seed {seed}\")\n",
    "\n",
    "        pi_1 = Policy(state_size=obs_dim, action_size=action_dim).to(device)\n",
    "        opt1 = torch.optim.Adam(pi_1.parameters(), lr=lr_oppo)\n",
    "\n",
    "\n",
    "        scores_oppo = OPPO_update(\n",
    "            policy          = pi_1,\n",
    "            optimizer       = opt1,\n",
    "            env             = env,\n",
    "            baseline        = baseline,\n",
    "            n_episodes      = n_episodes,\n",
    "            max_t           = max_t,\n",
    "            gamma           = gamma,      # you can reuse your PPO γ\n",
    "            print_every     = print_every,\n",
    "            early_stop      = False,\n",
    "            seed            = seed,\n",
    "            target_score    = target_score,\n",
    "            env_name        = env_name,\n",
    "            display_every   = False\n",
    "        )\n",
    "\n",
    "\n",
    "        pi_2 = load_policy(f\"pi2_ref_{env_name}_seed_{seed}.pth\", obs_dim, action_dim, device)\n",
    "    Policy_list.append([pi_1, pi_2])\n",
    "    # print(\"------------------------------------------------------\")\n",
    "\n",
    "print(\"=================================\")\n",
    "# ── Evaluate the policies ──\n",
    "for seed, (pi_1, pi_2) in zip(seeds_list, Policy_list):\n",
    "    \n",
    "    print(f\"Evaluating pi_1 and pi_2 with {seed=} over {n_eval} episodes\")\n",
    "    # Evaluate the policies\n",
    "    seed_eval = seed+32\n",
    "    _, pi1_rewards = evaluate_policy(pi_1, env, n_episodes=n_eval, seed=seed_eval)\n",
    "    _, pi2_rewards = evaluate_policy(pi_2, env, n_episodes=n_eval, seed=seed_eval)\n",
    "    print(\"Evaluations over\", n_eval, \"episodes done for both policies, using the seed {seed}\")\n",
    "    print(f\"pi_1({seed}) \\t mean reward: \\t {np.mean(pi1_rewards):.2f} ± {np.std(pi1_rewards):.2f}\")\n",
    "    print(f\"pi_2({seed}) \\t mean reward: \\t {np.mean(pi2_rewards):.2f} ± {np.std(pi2_rewards):.2f}\")\n",
    "\n",
    "    if plot_scores:\n",
    "        plot_Scores(\n",
    "            pi1_rewards,\n",
    "            pi2_rewards,\n",
    "        )\n",
    "    print(\"------------------------------------------------------\")\n",
    "\n",
    "\n",
    "print(\"=================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e8651d",
   "metadata": {},
   "source": [
    "### Now the Creation/loading of $\\pi_{ref}^{1|2}$ is done\n",
    "***\n",
    "## We will implement the part RLHF (PPO nik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cabb19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading policy trained with RLHF\n",
      "Loading policy trained with RLHF\n",
      "Loading policy trained with RLHF\n",
      "=================================\n",
      "Evaluations over 100 episodes done for the 3 policies, using the seed {seed}\n",
      "pi_1(35) \t mean reward: \t 188.64 ± 23.17\n",
      "pi_2(35) \t mean reward: \t 110.04 ± 53.85\n",
      "pi_{RLHF}(35) \t mean reward: \t 186.77 ± 22.00\n",
      "------------------------------------------------------\n",
      "Evaluations over 100 episodes done for the 3 policies, using the seed {seed}\n",
      "pi_1(42) \t mean reward: \t 187.78 ± 31.87\n",
      "pi_2(42) \t mean reward: \t 118.25 ± 59.47\n",
      "pi_{RLHF}(42) \t mean reward: \t 172.92 ± 44.14\n",
      "------------------------------------------------------\n",
      "Evaluations over 100 episodes done for the 3 policies, using the seed {seed}\n",
      "pi_1(100) \t mean reward: \t 190.67 ± 22.10\n",
      "pi_2(100) \t mean reward: \t 115.44 ± 48.00\n",
      "pi_{RLHF}(100) \t mean reward: \t 187.74 ± 28.16\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# List of hyperparameters\n",
    "lr_RLHF = 0.009\n",
    "K       = 200\n",
    "beta    = 0.5\n",
    "\n",
    "# Parameters for the RLHF part\n",
    "load_RM = True\n",
    "load_rlhf_policy = True\n",
    "pi_ref_rlhf = 1\n",
    "load_pi2_start_rlhf = True\n",
    "plot_scores = False\n",
    "n_eval = 100\n",
    "\n",
    "Policy_rlhf_list = []\n",
    "\n",
    "for seed, (pi_1, pi_2) in zip(seeds_list, Policy_list):\n",
    "    # Create the policy that will be trained with RLHF\n",
    "    policy_RLHF = Policy(state_size= obs_dim, action_size= action_dim).to(device)\n",
    "    if load_rlhf_policy:\n",
    "        policy_RLHF = load_policy(fr\"pi_RLHF_{env_name}_seed_{seed}_beta{beta}_K{K}.pth\", obs_dim, action_dim, device)\n",
    "        print(f\"Loading policy trained with RLHF for {seed=}\")\n",
    "    else:\n",
    "        # Creating the preference pairs\n",
    "        prefs = sample_preference_pairs(pi_1, pi_2, env, K=K)\n",
    "        print(f\"Collected {K} preference pairs for {seed=}.\")\n",
    "        \n",
    "        # Create the reward model\n",
    "        reward_model = RewardModel(state_dim=obs_dim, action_dim=action_dim).to(device)\n",
    "        \n",
    "        if load_RM:\n",
    "            print(f\"Loading reward model (MERCI Youssef)\")\n",
    "            reward_model.load_state_dict(torch.load(r\"reward_model_youss.pth\"))\n",
    "        else:\n",
    "            # Train the reward model\n",
    "            print(f\"Training reward model trained on {K} preference pairs\")\n",
    "\n",
    "        policy_RLHF = Policy(state_size= obs_dim, action_size= action_dim).to(device)\n",
    "        if load_pi2_start_rlhf: policy_RLHF.load_state_dict(torch.load(f\"pi2_ref_{env_name}_seed_{seed}.pth\"))\n",
    "        opt_RLHF    = torch.optim.Adam(policy_RLHF.parameters(), lr=lr_RLHF)\n",
    "    \n",
    "        policy_ref = pi_1 if pi_ref_rlhf == 1 else pi_2\n",
    "\n",
    "        print(f\"Training policy with RLHF using pi_{pi_ref_rlhf} as reference policy\")\n",
    "        train_policy_from_rollouts_n_updates(policy_RLHF, policy_ref, reward_model, env, opt_RLHF, N=20, K=K, max_steps=500, beta=beta)\n",
    "        torch.save(policy_RLHF.state_dict(), fr\"pi_RLHF_{env_name}_seed_{seed}_beta{beta}_K{K}.pth\")\n",
    "        print(fr\"Saved final policy as pi_RLHF_{env_name}_seed_{seed}_beta{beta}_K{K}.pth\")\n",
    "    Policy_rlhf_list.append(policy_RLHF)\n",
    "print(\"=================================\")\n",
    "\n",
    "\n",
    "\n",
    "for seed, (pi_1, pi_2), pi_rlhf in zip(seeds_list, Policy_list, Policy_rlhf_list):\n",
    "\n",
    "    # print(fr\"Evaluating pi_1, pi_2 and pi_rlhf with seed {seed}\")\n",
    "    \n",
    "    seed_eval = seed+32\n",
    "    _, pi1_rewards      = evaluate_policy(pi_1,     env, n_episodes=n_eval, seed=seed_eval)\n",
    "    _, pi2_rewards      = evaluate_policy(pi_2,     env, n_episodes=n_eval, seed=seed_eval)\n",
    "    _, pi_rlhf_rewards  = evaluate_policy(pi_rlhf,  env, n_episodes=n_eval, seed=seed_eval)\n",
    "    \n",
    "    print(\"Evaluations over\", n_eval, \"episodes done for the 3 policies, using the seed {seed}\")\n",
    "    print(f\"pi_1({seed}) \\t mean reward: \\t {np.mean(pi1_rewards):.2f} ± {np.std(pi1_rewards):.2f}\")\n",
    "    print(f\"pi_2({seed}) \\t mean reward: \\t {np.mean(pi2_rewards):.2f} ± {np.std(pi2_rewards):.2f}\")\n",
    "    print(f\"pi_RLHF({seed}) \\t mean reward: \\t {np.mean(pi_rlhf_rewards):.2f} ± {np.std(pi_rlhf_rewards):.2f}\")\n",
    "    if plot_scores:\n",
    "        plot_scores_RLHF(pi2_rewards, pi1_rewards, pi_rlhf_rewards, algo=\"RLHF\")\n",
    "\n",
    "    print(\"------------------------------------------------------\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46f1f45",
   "metadata": {},
   "source": [
    "***\n",
    "## Bonne chance j'ai pas touché la suite...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f7d772",
   "metadata": {},
   "source": [
    "Averaging over the different seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b211fd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_pi1=[f\"pi1_ref_{env_name}_seed_{seed}.pth\"]\n",
    "# file_pi2=[f\"pi2_ref_{env_name}_seed_{seed}.pth\"]\n",
    "# file_pi_DPO=[f\"pi_DPO_oppo_{env_name}_seed_{seed}.pth\",f\"pi_DPO_oppo_{env_name}_seed_{seed}_K200.pth\",f\"pi_DPO_oppo_{env_name}_seed_{seed}_beta0.005.pth\",f\"pi_DPO_oppo_{env_name}_seed_{seed}_beta1_invverted.pth\"]\n",
    "\n",
    "# results=evaluate_all_policies(env, seeds_list, env_name, num_episodes=50, device=device)\n",
    "\n",
    "# # Example labels: 'pi_DPO_', 'pi1', 'pi2'\n",
    "# print(results.keys())\n",
    "# reward_hist_init = results[\"pi2\"][\"graph\"]\n",
    "# reward_hist_ref = results[\"pi1\"][\"graph\"]\n",
    "# reward_hist_RLHF = results[\"pi_RLHF_pi_RLHF_CartPole-v0_seed_35_beta0.5_K200\"][\"graph\"]\n",
    "\n",
    "# plot_suboptimality_three_policies(reward_hist_RLHF, reward_hist_init, reward_hist_ref, max_reward=200,algo=\"RLHF\")\n",
    "\n",
    "\n",
    "#================================================\n",
    "#------------------------------------\n",
    "# reward_hist_dpo = results[\"pi_DPO_pi_DPO_CartPole-v0_seed_35_K200\"][\"graph\"]\n",
    "\n",
    "# plot_suboptimality_three_policies(reward_hist_dpo, reward_hist_init, reward_hist_ref, max_reward=200)\n",
    "# reward_hist_dpo = results[\"pi_DPO_pi_DPO_CartPole-v0_seed_35_beta0.005\"][\"graph\"]\n",
    "\n",
    "# plot_suboptimality_three_policies(reward_hist_dpo, reward_hist_init, reward_hist_ref, max_reward=200)\n",
    "# reward_hist_dpo = results[\"pi_DPO_pi_DPO_CartPole-v0_seed_35_beta0.5_K10\"][\"graph\"]\n",
    "\n",
    "# plot_suboptimality_three_policies(reward_hist_dpo, reward_hist_init, reward_hist_ref, max_reward=200)\n",
    "\n",
    "\n",
    "# reward_hist_dpo = results[\"pi_DPO_pi_DPO_CartPole-v0_seed_35_beta1_inverted\"][\"graph\"]\n",
    "# reward_hist_init = results[\"pi1\"][\"graph\"]\n",
    "# reward_hist_ref = results[\"pi2\"][\"graph\"]\n",
    "# plot_suboptimality_three_policies(reward_hist_dpo, reward_hist_init, reward_hist_ref, max_reward=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
